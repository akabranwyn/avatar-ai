* Lead in from sandman's take on his own vid
<iframe width="560" height="315" src="https://www.youtube.com/embed/DAcn9bAZ114" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

* cut to sandman's take in transmaxxing
<iframe width="560" height="315" src="https://www.youtube.com/embed/mxw6p6-naJw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>


### MGTOW's best-case arrangement with "females"

The manosphere seems to think that if they're able to hold out long enough their dream vision of a perfect partner can become a reality - A female presence who:
- isn't interested in resource extraction
- can fit whatever definition of beauty and engagement they see fit
- and can have sex without any conflicting emotional reciprocation

So any relationship in this context is decidedly one-sided. In fact this aligns with what a lot of the manosphere, MGTOW included, have advocated for. *geomaxxing, something else here*

In general MGTOW's attitude towards relationships is that you will always need to be guarded and ready to "go your own way" as soon as something is not favorable towards you in this arrangement. It is the prevention of any anchors and attachments that may force a male to sacrifice in order to maintain the advantages they may get from the arrangement. Deep emotional bonds are a weakness that can lead to manipulation or an uneven power structure unfavorable to the men. Are these men so emotionally drained that any relationship is viewed as parasitic?

A robotic companion in this sense fits a perfect bill; of course, this is because "robot" (from greek? slave) is a purely tunable and programmable entity that can exceed at a form of mimicry and an analog for love that the MGTOW followers idealize.

Possibly ironic, since a robot can also be defined as 
```
    a person who behaves in a mechanical or unemotional manner
```
*hmmmm*

But apart from the absurdity and sadness of a life with only a shallow emotional connection, what about the practicality? 

### The rise of LLM


In the past year alone we've seen the explosion of Large Language Models such as ChatGPT (based on GPT-3.5 wrapped with an API) onto the scene, creating huge impact as well as headlines.

* Clips, ChatGPT coverage*
* GPT-2 releases 05/19, GPT-3 releases 05/20, GPT-3.5 releases Nov/22 *

With the arrival of GPT-4 just under a year later in March 2023 (with much thanks to reinforced learnings with ChatGPT's feedback and collosal success), the promise of true conversational AI is just around the corner, right? *nuclear fusion a decade away*

Consider this though; any large language model (in this case, an inference model) is simply a set of connected layers providing a mathematical (and probabilistic) pathway from one dissected input such as text, images, or speech to an appropriately formatted response. Now the way this model is generated takes into account a wide range of data. This can be targetted to specific situations of course, but the most appealing form is to be able to be as generic as possible, and able to respond to the widest array of possible input prompts. This is what you'd want in your partner, right? The same formulaic responses would just be boring.

### About GPT-4

https://openai.com/research/gpt-4
- GPT-4 is successful in these situations : written responses that can be answered through rote repitition, recitation of sources, or can be deemed as otherwise objectively correct
- GPT 4 is unsuccessful when: reasoned problem solving and solution thinking are required (creating complex functional code from a basic prompt; complex multistep math (calculus); ***creative or non-formulaic writing***)
- GPT-4 also suffers from hallucinations : creating false answers through ignoring key prompt details; citing incorrect sources (or made up ones); ***deducing incorrect answers from conflicting information***; creating responses that look correct but are false.

### The Linear Projection fallacy

Now let's switch gears, what's a reasonable timeline that people are willing to wait for this sort of objectified and possibly formulaic relationship? 3 years? 5? 10?

The linear projection fallacy is: 

```
*Define linear projection fallacy* 
The error of presuming that future change will be a simple and steady extension of past trends â€“ i.e., most disciplines envisioning futures with only minor changes as compared to the present moment.
```

Practically speaking, technology advances what seems like years in a day, and what seems like a day in years. Before GPT-3, development began ((research))

### What are you Really Waiting for?

What does a normal product development cycle look like? Let's say we start today from the best tech, rather, the best sex robot tech. 

(realdolls?)

Now, add in some capability to interact, even a year from now let's call it GPT-5; What are we looking at? 

*Synthesized voice from chatgpt output, let's say human edited, static doll*

Is this really appealing as a companion - even just for sex? Let's move forward another year - we refine the features, add some incredible advancements in robotics, we get some ... convincing facial movement and upgrade to GPT-6! This time with realistic pillow talk AND moaning in 30 different languages. Formulaic responses sure, but at least this has a learning capability for training to a much more personalized response set based on previous conversations. She remembers you had that project due and wishes she could actually rub your shoulders.

No? Still not appealing? 5 years - incredible boston robotics level of articulation has reached the consumer! GPT-8 is completely conversational. Althouth optimistic, the holy grail of companionship is here! 

After 5 years, the most basic level of companionship (not just sex of course) is available for... top price.

Was it really worth the wait? Is a 5 year opportunity cost, 5 years of actual happiness, worth an optimistic perfect companion? 

### Conclusion

To be fair, Sandman and others debunk this notion on the basis of time to market, but present equally ludicrous reasons for doing so. Clones, made to order humans, and grooming perfect partners have not just an entirely different set of practical concerns but huge ethical ones as well.

What these men who are trying to sell this vision for the manosphere are really selling is a form of futuristic hope, with a touch of self-servience. A robotic, unemotional life (mechanical or biological) isn't appealing to every young man who is looking for their path to happiness - by definition you are giving up any sort of deeper emotional connection to avoid the possibility of hurt or manipulation. There are of course other options for finding happiness - some that are just as poorly thought out (again courtesy of MGTOW) and others that require a pragmatic evaluation of the actual feelings, needs, and desires that these men are experiencing. Notably, is the male social role really what they want for their life.

*Callouts to helpful people, Kattt, links to discords in the description*
